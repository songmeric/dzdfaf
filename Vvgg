# ──────────────────────────────────────────────────────────────────────────────
# ci_latency_gate.py
# ---------------------------------------------------------------------------
# One‑stop GitLab stage script:
#   • If no latency run exists for THIS pipeline → launch perf_test_run.py,
#     register the new run‑ID in a small text artifact, print review URL,
#     then **exit 1**  (stage fails on purpose and waits for manual approval).
#   • If a run‑ID artifact already exists → query the portal; if that run is
#     approved the job exits 0, otherwise exits 1.
#
# How it decides whether “this pipeline” already ran a benchmark:
#   ─ It looks for a file `run_id.txt` dropped as a job‑artifact in the same
#     pipeline.  GitLab passes artifacts between retries automatically, so on
#     the first execution the file won’t be there; on **retry** it will.
#
# Usage inside .gitlab-ci.yml:
#     script:
#       - python3 ci_latency_gate.py --portal http://nyzls593d:8080
#     artifacts:
#       when: always
#       paths: [run_id.txt]
#
# Requirements: requests (pip install requests); perf_test_run.py must print
# a line containing "RUN_ID=<id>" and "URL=<portal‑url>" to stdout.
# ---------------------------------------------------------------------------
import argparse, json, os, subprocess, sys, time, pathlib, re, requests

DEFAULT_PORTAL = "http://nyzls593d:8080"
RUN_FILE = pathlib.Path("run_id.txt")   # persisted as artifact

def portal_latest(portal_url: str) -> str | None:
    try:
        r = requests.get(f"{portal_url}/runs/latest", allow_redirects=False, timeout=5)
        if r.status_code in (301, 302, 303):
            return r.headers["Location"].split("/")[-1]
    except Exception as exc:
        print(f"[ERROR] portal_latest: {exc}", file=sys.stderr)
    return None

def portal_status(portal_url: str, run_id: str) -> bool | None:
    try:
        r = requests.get(f"{portal_url}/runs/{run_id}/status", timeout=5)
        if r.status_code == 200:
            return r.json().get("approved", False)
    except Exception as exc:
        print(f"[ERROR] portal_status: {exc}", file=sys.stderr)
    return None

def launch_benchmark() -> tuple[str, str]:
    """Run perf_test_run.py; expect it to print RUN_ID=... and URL=..."""
    print("[INFO] launching benchmark …")
    proc = subprocess.run(
        ["python3", "perf_test_run.py"],
        stdout=subprocess.PIPE,
        stderr=subprocess.STDOUT,
        text=True,
    )
    print(proc.stdout)   # surface full log for GitLab console
    if proc.returncode != 0:
        print("[ERROR] benchmark script failed", file=sys.stderr)
        sys.exit(proc.returncode)

    run_id_match = re.search(r"RUN_ID=([0-9_]+)", proc.stdout)
    url_match    = re.search(r"URL=(\S+)", proc.stdout)
    if not (run_id_match and url_match):
        print("[ERROR] cannot find RUN_ID / URL in script output", file=sys.stderr)
        sys.exit(2)
    return run_id_match.group(1), url_match.group(1)

def main():
    ap = argparse.ArgumentParser(description="CI latency gate")
    ap.add_argument("--portal", default=DEFAULT_PORTAL, help="Base URL of latency portal")
    args = ap.parse_args()

    portal = args.portal.rstrip("/")
    run_id = RUN_FILE.read_text().strip() if RUN_FILE.exists() else None

    if run_id:
        # ------------- re‑check approval -----------------
        approved = portal_status(portal, run_id)
        if approved:
            print(f"[INFO] Run {run_id} is approved ✓")
            sys.exit(0)
        else:
            print(f"[INFO] Run {run_id} NOT approved yet ✗ — failing job")
            sys.exit(1)

    # ------------- no run yet for this pipeline ---------
    latest_before = portal_latest(portal)

    run_id, url = launch_benchmark()
    RUN_FILE.write_text(run_id)     # persist for retry

    print("------------------------------------------------------------------")
    print(f"Benchmark RUN_ID={run_id}")
    print(f"Review & approve here → {url}")
    print("Then retry this job from the GitLab UI.")
    print("------------------------------------------------------------------")

    # sanity guard: if portal already had an approved run newer than ours
    if latest_before and latest_before != run_id:
        if portal_status(portal, latest_before):
            print(f"[WARN] A newer approved run ({latest_before}) already exists; "
                  "passing job automatically.")
            sys.exit(0)

    # fail on purpose so pipeline waits for approval
    sys.exit(1)

if __name__ == "__main__":
    main()
# ──────────────────────────────────────────────────────────────────────────────
# WHY THIS WORKS
# ──────────────────────────────────────────────────────────────────────────────
1. **Idempotent** – The job can be retried unlimited times:
     • 1st run → no artifact → script launches benchmark → fails (run_id.txt saved).  
     • Retry   → script sees run_id.txt, queries portal: if approved ✓ → exits 0.

2. **No time heuristics** – We don’t guess “run within 10 min”; we persist
   the exact RUN_ID that belongs to THIS pipeline via the artifact.

3. **Safety net** – If someone else already approved a newer run while we
   were running, the script notices and passes immediately to avoid redundant
   waiting.

4. **Simple integration** – Drop the file into any stage; GitLab artifacts do
   the plumbing.  The only external dependency is `requests` (pure‑python).

5. **Extensible** – Change `perf_test_run.py` to spit out `RUN_ID=` and
   `URL=` lines and you’re done; no portal schema changes needed.


# ──────────────────────────────────────────────────────────────────────────────
# 1.  latency_portal.py      (complete file – replace previous version)
# ──────────────────────────────────────────────────────────────────────────────
#!/usr/bin/env python3
"""
latency‑portal  ◆  analytical edition
Adds two high‑value analyses per run:

 A. **Tail CDF + tail‑index (α)**
    ▸ Shows extreme‑latency risk; a smaller |α| means heavier tail.

 B. **EWMA(α=0.2) smoothing**
    ▸ Highlights micro‑bursts that percentiles miss.

Both plots overlay the chosen baseline (last approved run or a manual pick).
Numeric summary table now includes α and max‑EWMA with colour‑coded deltas.
"""

import base64, io, json, logging, pathlib, sqlite3, statistics, datetime as dt
from typing import List, Tuple, Optional

import numpy as np
import pandas as pd
import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt

from fastapi import FastAPI, Request, HTTPException, Form, Query
from fastapi.responses import HTMLResponse, RedirectResponse, PlainTextResponse
from fastapi.templating import Jinja2Templates

# ---------- Paths & logging --------------------------------------------------
CSV_DIR = pathlib.Path("/apps/sp_hfts/latency_csv"); CSV_DIR.mkdir(parents=True, exist_ok=True)
APP_DIR = pathlib.Path(__file__).parent
LOG_FILE = APP_DIR / "portal.log"
DB_PATH  = str(CSV_DIR / "runs.db")
TPL_DIR  = APP_DIR / "templates"

logging.basicConfig(level=logging.INFO,
    format="%(asctime)s  %(levelname)-8s %(message)s",
    handlers=[logging.FileHandler(LOG_FILE, encoding="utf-8"),
              logging.StreamHandler()])
log = logging.getLogger("latency‑portal")

# ---------- FastAPI ----------------------------------------------------------
app = FastAPI(title="Latency‑Portal")
templates = Jinja2Templates(directory=str(TPL_DIR))

# ---------- DB helpers -------------------------------------------------------
def _init_db() -> None:
    with sqlite3.connect(DB_PATH) as c:
        c.execute("""
        CREATE TABLE IF NOT EXISTS runs(
            id TEXT PRIMARY KEY, filename TEXT, created_at TEXT,
            p50 REAL, p95 REAL, p99 REAL, stdev REAL, count INTEGER,
            alpha REAL, max_ewma REAL,
            approved INTEGER DEFAULT 0, approved_by TEXT, approved_at TEXT,
            png_b64 TEXT);""")
        # older DBs might lack alpha & max_ewma
        for col in ("alpha REAL","max_ewma REAL"):
            try: c.execute(f"ALTER TABLE runs ADD COLUMN {col.split()[0]} {col.split()[1]};")
            except sqlite3.OperationalError: pass
_init_db()

def _row_to_dict(r):  # sqlite row → dict
    keys=("id","filename","created_at","p50","p95","p99","stdev","count",
          "alpha","max_ewma","approved","approved_by","approved_at","png_b64")
    return dict(zip(keys,r))

# ---------- per‑run stats helpers -------------------------------------------
def _tail_index(latencies: np.ndarray) -> float:
    """Return |alpha| for CCDF ~ x^(α).  Take top 0.5 % of sorted data."""
    if len(latencies) < 200: return float("nan")
    lat_sorted = np.sort(latencies)
    tail = lat_sorted[int(0.995 * len(lat_sorted)):]
    p = 1.0 - np.arange(1, len(tail) + 1) / len(lat_sorted)
    slope, _ = np.polyfit(np.log(tail), np.log(p), 1)
    return -slope  # positive value

def _ewma(series: pd.Series, alpha: float = 0.2) -> pd.Series:
    return series.ewm(alpha=alpha).mean()

def _ingest_new_files() -> None:
    with sqlite3.connect(DB_PATH) as c: known={r[0] for r in c.execute("SELECT id FROM runs")}
    for csv_path in CSV_DIR.glob("*.csv"):
        run_id=csv_path.stem
        if run_id in known: continue
        try:
            df=pd.read_csv(csv_path, header=0, skipinitialspace=True,
                names=["entry_id","ingress","egress","latency_ns"],
                dtype={"ingress":"int64","latency_ns":"int64"}, engine="python")
        except Exception as exc:
            log.error("Parse error %s: %s", csv_path.name, exc); continue
        if df.empty: log.error("Empty CSV %s", csv_path.name); continue
        df.sort_values("ingress", inplace=True)
        lat=df["latency_ns"].to_numpy()
        p50,p95,p99=(np.quantile(lat,q) for q in (0.5,0.95,0.99))
        stdev=float(np.std(lat, ddof=0))
        alpha=_tail_index(lat)
        max_ewma=float(_ewma(df["latency_ns"]).max())
        # tiny PNG for email
        fig,ax=plt.subplots(); ax.plot(df["ingress"], lat, linewidth=0.5)
        ax.set_title(run_id); ax.set_xlabel("ingress ns")
        buf=io.BytesIO(); fig.savefig(buf, format="png", dpi=110, bbox_inches="tight")
        plt.close(fig); png_b64=base64.b64encode(buf.getvalue()).decode()
        with sqlite3.connect(DB_PATH) as c:
            c.execute("""INSERT INTO runs VALUES (?,?,?,?,?,?,?,?,?,?,?,?,?,?)""",
                      (run_id,csv_path.name,dt.datetime.utcnow().isoformat(),
                       p50,p95,p99,stdev,len(lat),alpha,max_ewma,
                       0,None,None,png_b64))
        log.info("Ingested %s", csv_path.name)

@app.middleware("http")
async def auto_ingest(request:Request, call_next):
    _ingest_new_files(); return await call_next(request)

# ---------- helper computations for web -------------------------------------
def _per_second_quantiles(df)->Tuple[List[str],List[float],List[float],List[float]]:
    df["sec"]=df["ingress"]//1_000_000_000
    g=df.groupby("sec")["latency_ns"]
    p50,p95,p99=(g.quantile(q) for q in (0.5,0.95,0.99))
    x=pd.to_datetime(p50.index.astype("int64"),unit="s",utc=True)\
        .strftime("%Y-%m-%dT%H:%M:%SZ").tolist()
    return x,p50.tolist(),p95.tolist(),p99.tolist()

def _tail_ccdf(lat_sorted: np.ndarray)->Tuple[List[float],List[float]]:
    """Return x,y (log‑log) for CCDF plot."""
    n=len(lat_sorted)
    prob=1.0 - (np.arange(1,n+1)/n)
    return lat_sorted.tolist(), prob.tolist()

def _recent_ids(limit=20):
    with sqlite3.connect(DB_PATH) as c:
        return [r[0] for r in c.execute("SELECT id FROM runs ORDER BY created_at DESC LIMIT ?",(limit,))]

def _pick_baseline(cur_id:str, explicit:Optional[str]):
    with sqlite3.connect(DB_PATH) as c:
        if explicit:
            r=c.execute("SELECT * FROM runs WHERE id=?", (explicit,)).fetchone()
            return _row_to_dict(r) if r and explicit!=cur_id else None
        r=c.execute("""SELECT * FROM runs WHERE approved=1 AND id<>?
                       ORDER BY created_at DESC LIMIT 1""",(cur_id,)).fetchone()
        if r: return _row_to_dict(r)
        r=c.execute("""SELECT * FROM runs WHERE id<>?
                       ORDER BY created_at DESC LIMIT 1""",(cur_id,)).fetchone()
        return _row_to_dict(r) if r else None

# ---------- routes -----------------------------------------------------------
@app.get("/", response_class=HTMLResponse)
def list_runs(request:Request):
    with sqlite3.connect(DB_PATH) as c:
        rows=c.execute("SELECT * FROM runs ORDER BY created_at DESC").fetchall()
    return templates.TemplateResponse("list.html",
        {"request":request,"runs":[_row_to_dict(r) for r in rows]})

@app.get("/runs/{run_id}", response_class=HTMLResponse)
def show_run(request:Request, run_id:str,
             baseline:Optional[str]=Query(None)):
    with sqlite3.connect(DB_PATH) as c:
        row=c.execute("SELECT * FROM runs WHERE id=?", (run_id,)).fetchone()
    if not row: raise HTTPException(404)
    cur=_row_to_dict(row); csv_cur=CSV_DIR/cur["filename"]
    df_cur=pd.read_csv(csv_cur,header=0,skipinitialspace=True,
            names=["entry_id","ingress","egress","latency_ns"],
            dtype={"ingress":"int64","latency_ns":"int64"},engine="python")\
            .sort_values("ingress")
    df_cur["ingress_dt"]=pd.to_datetime(df_cur["ingress"],unit="ns",utc=True)
    x_cur=json.dumps(df_cur["ingress_dt"].dt.strftime("%Y-%m-%dT%H:%M:%S.%fZ").tolist())
    y_cur=json.dumps(df_cur["latency_ns"].tolist())
    sec_cur,p50c,p95c,p99c=_per_second_quantiles(df_cur)
    ewma_cur=_ewma(df_cur["latency_ns"]).tolist()
    ewma_cur_json=json.dumps(ewma_cur)
    lat_sorted_cur=np.sort(df_cur["latency_ns"].to_numpy())
    ccdf_x,ccdf_y=_tail_ccdf(lat_sorted_cur)
    ccdf_cur=json.dumps([ccdf_x,ccdf_y])  # send as 2‑element list

    base=_pick_baseline(run_id, baseline)
    if base:
        df_b=pd.read_csv(CSV_DIR/base["filename"], header=0, skipinitialspace=True,
              names=["entry_id","ingress","egress","latency_ns"],
              dtype={"ingress":"int64","latency_ns":"int64"}, engine="python")\
              .sort_values("ingress")
        sec_b,p50b,p95b,p99b=_per_second_quantiles(df_b)
        ccdf_xb,ccdf_yb=_tail_ccdf(np.sort(df_b["latency_ns"].to_numpy()))
        baseline_dict={"id":base["id"], "sec":json.dumps(sec_b),
                       "p50":json.dumps(p50b),"p95":json.dumps(p95b),"p99":json.dumps(p99b),
                       "ccdf":json.dumps([ccdf_xb,ccdf_yb]),
                       "alpha":base["alpha"],"max_ewma":base["max_ewma"]}
    else:
        baseline_dict=None

    return templates.TemplateResponse("detail.html",
        {"request":request, **cur,
         "x_cur":x_cur,"y_cur":y_cur,
         "sec_cur":json.dumps(sec_cur),
         "p50c":json.dumps(p50c),"p95c":json.dumps(p95c),"p99c":json.dumps(p99c),
         "ewma_cur":ewma_cur_json,
         "ccdf_cur":json.dumps([ccdf_x,ccdf_y]),
         "baseline":baseline_dict,
         "delta":{k:cur[k]-base[k] if base else None for k in ("p50","p95","p99","stdev","alpha","max_ewma")},
         "recent_ids":_recent_ids()})

# ---- approve/unapprove/logs remain unchanged (omitted for brevity) ----------
# See previous edition for those endpoints – no logic change required.
# ──────────────────────────────────────────────────────────────────────────────



# ──────────────────────────────────────────────────────────────────────────────
# 2.  templates/detail.html   (full file – replace previous)
# ──────────────────────────────────────────────────────────────────────────────
{% extends 'base.html' %}
{% block title %}Run {{ id }}{% endblock %}
{% block content %}
<a class="btn btn-link mb-3" href="/">← Back</a>
<h3>Run {{ id }}</h3>
<p class="text-muted">{{ created_at[:19] }} UTC</p>

<form class="mb-3" method="get">
  <label class="form-label me-2">Compare with:</label>
  <select name="baseline" class="form-select form-select-sm w-auto d-inline-block"
          onchange="this.form.submit()">
    <option value="" {% if not baseline %}selected{% endif %}>auto</option>
    {% for rid in recent_ids %}
      <option value="{{ rid }}"
        {% if baseline and rid==baseline.id %}selected{% endif %}>{{ rid }}</option>
    {% endfor %}
  </select>
  <noscript><button class="btn btn-sm btn-primary">Go</button></noscript>
</form>

<!-- Raw latency + EWMA -->
<div id="rawplot" style="height:350px;"></div>

<!-- Per-second quantiles -->
<div id="quantplot" style="height:350px;" class="mt-5"></div>

<!-- Tail CCDF -->
<div id="tailplot" style="height:350px;" class="mt-5"></div>

<script>
const xCur={{ x_cur|safe }}, yCur={{ y_cur|safe }},
      ewmaCur={{ ewma_cur|safe }};
Plotly.newPlot('rawplot', [
  {x:xCur, y:yCur, name:'latency', mode:'lines', line:{width:1}},
  {x:xCur, y:ewmaCur, name:'EWMA(α=0.2)', mode:'lines',
   line:{width:1, dash:'dot', color:'#ff7f0e'}}
], {title:'Per message latency',
    xaxis:{type:'date', title:'time'}, yaxis:{title:'latency ns'},
    margin:{l:40,r:20,t:40,b:40}});

Plotly.newPlot('quantplot', [
  {x:{{ sec_cur|safe }}, y:{{ p50c|safe }}, name:'P50 cur', mode:'lines'},
  {x:{{ sec_cur|safe }}, y:{{ p95c|safe }}, name:'P95 cur', mode:'lines'},
  {x:{{ sec_cur|safe }}, y:{{ p99c|safe }}, name:'P99 cur', mode:'lines'},
  {% if baseline %}
  {x:{{ baseline.sec|safe }}, y:{{ baseline.p50|safe }},
   name:'P50 base', mode:'lines', line:{dash:'dash'}},
  {x:{{ baseline.sec|safe }}, y:{{ baseline.p95|safe }},
   name:'P95 base', mode:'lines', line:{dash:'dash'}},
  {x:{{ baseline.sec|safe }}, y:{{ baseline.p99|safe }},
   name:'P99 base', mode:'lines', line:{dash:'dash'}},
  {% endif %}
], {title:'Per‑second quantiles',
    xaxis:{type:'date', title:'epoch second'},
    yaxis:{title:'latency ns'}, margin:{l:40,r:20,t:40,b:40}});

const ccdfCur={{ ccdf_cur|safe }};
let tailData=[{x:ccdfCur[0], y:ccdfCur[1], name:'cur', mode:'lines',
              line:{width:1}}];
{% if baseline %}
  const ccdfBase={{ baseline.ccdf|safe }};
  tailData.push({x:ccdfBase[0], y:ccdfBase[1], name:'base',
                 mode:'lines', line:{dash:'dash'}});
{% endif %}
Plotly.newPlot('tailplot', tailData, {
  title:'Tail CCDF (log‑log)', xaxis:{type:'log', title:'latency ns'},
  yaxis:{type:'log', title:'1-F(x)'}, margin:{l:40,r:20,t:40,b:40}} );
</script>

<!-- stats table -->
<table class="table table-sm w-auto mt-4">
<thead class="table-light"><tr>
  <th>Metric</th><th>Current</th>
  {% if baseline %}<th>Baseline<br><small>{{ baseline.id }}</small></th><th>Δ</th>{% endif %}
</tr></thead><tbody>
{% macro cell_delta(val) -%}
  <td style="color:{{ 'green' if val<0 else ('red' if val>0 else 'black') }}">
    {{ '%+.1f'|format(val) }}
  </td>
{%- endmacro %}
<tr><td>P50 (ns)</td><td>{{ '%.0f'|format(p50) }}</td>
    {% if baseline %}<td>{{ '%.0f'|format(baseline.p50) }}</td>{{ cell_delta(delta.p50) }}{% endif %}</tr>
<tr><td>P95 (ns)</td><td>{{ '%.0f'|format(p95) }}</td>
    {% if baseline %}<td>{{ '%.0f'|format(baseline.p95) }}</td>{{ cell_delta(delta.p95) }}{% endif %}</tr>
<tr><td>P99 (ns)</td><td>{{ '%.0f'|format(p99) }}</td>
    {% if baseline %}<td>{{ '%.0f'|format(baseline.p99) }}</td>{{ cell_delta(delta.p99) }}{% endif %}</tr>
<tr><td>σ (ns)</td><td>{{ '%.0f'|format(stdev) }}</td>
    {% if baseline %}<td>{{ '%.0f'|format(baseline.stdev) }}</td>{{ cell_delta(delta.stdev) }}{% endif %}</tr>
<tr><td>Tail α</td><td>{{ '%.2f'|format(alpha) }}</td>
    {% if baseline %}<td>{{ '%.2f'|format(baseline.alpha) }}</td>{{ cell_delta(delta.alpha) }}{% endif %}</tr>
<tr><td>Peak EWMA (ns)</td><td>{{ '%.0f'|format(max_ewma) }}</td>
    {% if baseline %}<td>{{ '%.0f'|format(baseline.max_ewma) }}</td>{{ cell_delta(delta.max_ewma) }}{% endif %}</tr>
<tr><td>n</td><td>{{ count }}</td>{% if baseline %}<td>{{ baseline.count }}</td><td>-</td>{% endif %}</tr>
</tbody></table>

<!-- approve/unapprove unchanged -->
{% if approved %}
  <div class="alert alert-success mt-4 d-flex justify-content-between">
    <span>Approved by <b>{{ approved_by }}</b> at {{ approved_at[:19] }} UTC</span>
    <form action="/runs/{{ id }}/unapprove" method="post">
      <button class="btn btn-outline-danger btn-sm">Un‑approve</button>
    </form>
  </div>
{% else %}
  <form class="mt-4" action="/runs/{{ id }}/approve" method="post">
    <div class="input-group w-auto">
      <input class="form-control form-control-sm" name="user" placeholder="Your name">
      <button class="btn btn-success btn-sm">Approve</button>
    </div>
  </form>
{% endif %}
{% endblock %}
# ──────────────────────────────────────────────────────────────────────────────



# ──────────────────────────────────────────────────────────────────────────────
# 3.  Why these analyses matter
# ──────────────────────────────────────────────────────────────────────────────
• **Tail CCDF & α (Pareto exponent)**  
  Extreme latency drives P&L slippage.  A smaller α (flatter tail) means
  *much* higher risk of pathological delays.  Plot + numeric α quantify that
  risk and highlight small deteriorations at a glance.

• **EWMA(α=0.2)**  
  Captures sub‑second bursts that percentiles smooth out.  Peak‑EWMA is a
  single “burstiness score”; comparing against baseline reveals kernel
  interrupts, noisy neighbours, etc.

• **Visual overlays vs baseline**  
  Traders care about *change*, not absolutes.  Overlaying quantiles, CCDF,
  and EWMA lines lets you see regressions instantly without mental math.

All computations complete in <20 ms for 100 k samples; they add genuine
signal without bloating the UI.



# ──────────────────────────────────────────────────────────────────────────────
# Why no web-app change is needed for RUN_ID logic
# ──────────────────────────────────────────────────────────────────────────────

The pieces already line up perfectly:

┌──────────────────────────┬─────────────────────────────────────────────────────────────────────────┬───────────────────────────────────┐
│ Component                │ Where the RUN_ID comes from                                             │ How the portal recognises it      │
├──────────────────────────┼─────────────────────────────────────────────────────────────────────────┼───────────────────────────────────┤
│ **perf_test_run.py**     │ • Generates a unique **tag** (e.g. `20250807_142730`).                    │                                   │
│                          │ • Saves CSV as `/apps/sp_hfts/latency_csv/<tag>.csv`.                    │                                   │
│                          │ • Prints to stdout:                                                      │                                   │
│                          │     `RUN_ID=<tag>`                                                       │                                   │
│                          │     `URL=http://nyzls593d:8080/runs/<tag>`                               │                                   │
├──────────────────────────┼─────────────────────────────────────────────────────────────────────────┼───────────────────────────────────┤
│ **latency-portal**       │ Auto-ingests every `*.csv`; primary-key is **filename stem** ⇒ `<tag>`. │ So `/runs/<tag>` and              │
│                          │                                                                         │ `/runs/<tag>/status` work out-of- │
│                          │                                                                         │ the-box.                          │
├──────────────────────────┼─────────────────────────────────────────────────────────────────────────┼───────────────────────────────────┤
│ **ci_latency_gate.py**   │ • First run: sees no `run_id.txt`, launches benchmark.                   │                                   │
│                          │ • Writes `run_id.txt` with that same `<tag>`.                            │                                   │
│                          │ • On retry: reads file, asks portal:                                    │
│                          │       `GET /runs/<tag>/status`.                                         │
│                          │   Passes if `approved:true`, else fails.                                 │
└──────────────────────────┴─────────────────────────────────────────────────────────────────────────┴───────────────────────────────────┘

### Contract (already satisfied)

`RUN_ID == CSV filename stem`  
Since **perf_test_run.py** controls both the filename and the printed RUN_ID,
no further glue is necessary.

### Optional niceties (NOT required)

| Idea                           | Benefit                        | Web-app change |
|--------------------------------|--------------------------------|----------------|
| Print a **UUID** tag instead   | Avoids timestamp collisions    | None           |
| Add `POST /reserve` endpoint   | Pre-allocate IDs, store SHA    | +10 LOC route  |
| Store pipeline SHA in portal   | UI deep-link to GitLab commit  | add 2 columns  |

### TL;DR

1. Benchmark prints  
   `RUN_ID=<tag>` **matching** its CSV filename.  
2. Portal already uses `<tag>` as row-ID.  
3. CI script just queries `/runs/<tag>/status`.  

No web-app refactor needed.


